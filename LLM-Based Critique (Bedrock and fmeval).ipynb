{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6440ccaa-cf89-4ab5-a367-4e2a9ba21c68",
   "metadata": {},
   "source": [
    "# Fully custom LLM evaluation with Amazon Bedrock and fmeval\n",
    "\n",
    "> *This notebook has been tested in the Python 3 kernel of SageMaker Studio JupyterLab (Distribution v1.6)*\n",
    "\n",
    "Automating the evaluation of LLMs is useful (to accelerate prompt engineering and solution optimization), but difficult (because the models output natural language text).\n",
    "\n",
    "The [open-source `fmeval` library](https://github.com/aws/fmeval) provides a range of evaluation algorithms, metrics, and integrations, and underpins the native automated foundation model evaluation capabilities offered [in Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation.html) and [in Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-foundation-model-evaluate.html).\n",
    "\n",
    "Recently, **LLM-based** automated evaluation procedures (that actually use one or more evaluator LLMs to judge the output of a candidate LLM) have shown popularity - with options included in tools like [LlamaIndex](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/#response-evaluation) and [Ragas](https://docs.ragas.io/en/latest/concepts/metrics/critique.html). However, at the time of writing, `fmeval`s built-in evaluation algorithms do **not** include any LLM-critique-based methods.\n",
    "\n",
    "This notebook shows a method to evaluate an LLM's accuracy for in-context question answering (with reference to known \"ground truth\" answers), using an evaluator LLM to determine whether the candidate model's response is in line with the ground truth. We show how this can be achieved within the context of `fmeval` (by providing a custom evaluation algorithm through the fmeval API), and explore how the LLM-based and built-in `QAAccuracy` metrics differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d159511-97d2-46d6-839e-1c3571213838",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "The [fmeval library](https://github.com/aws/fmeval) is not installed on SageMaker Studio JupyterLab kernels by default, so we'll first need to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7747a9-7070-4475-8bde-2bd10e5e8396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install \"fmeval>=1.0,<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e6ac5-45e3-4d56-acbf-945372d12ef8",
   "metadata": {},
   "source": [
    "You'll also need to **enable access to Anthropic Claude v3 (Haiku or Sonnet)** in Amazon Bedrock:\n",
    "\n",
    "1. Select an [AWS Region where the model is available](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) (which doesn't have to be the same region as you deployed this notebook). For e.g. `us-west-2`\n",
    "2. [Enable access to the Claude model in that region](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) through the Amazon Bedrock Console.\n",
    "3. Grant [Bedrock IAM permissions to your SageMaker notebook execution role](https://docs.aws.amazon.com/bedrock/latest/userguide/api-setup.html#api-using-sage) to be able to invoke the model from here in the notebook.\n",
    "\n",
    "Configure your Bedrock region, candidate & evaluator model ID, and Amazon S3 bucket in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7284a7a6-9e79-4b2e-8891-9bc6897fd7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Python Built-Ins:\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import os\n",
    "from string import Template\n",
    "from typing import Any, Callable, List, Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.eval_algorithms.qa_accuracy import QAAccuracy\n",
    "from fmeval.model_runners.bedrock_model_runner import BedrockModelRunner\n",
    "import pandas as pd  # Utilities for working with dataframes (tabular data)\n",
    "import sagemaker  # Amazon SageMaker high-level SDK\n",
    "\n",
    "\n",
    "BEDROCK_REGION = boto3.Session().region_name  # Override this with e.g. \"us-east-1\" if you need\n",
    "CANDIDATE_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "EVAL_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "bucket_name = sagemaker.Session().default_bucket()  # (Or a custom S3 bucket if you prefer)\n",
    "prefix = \"llm-eval/demo/squad\"\n",
    "\n",
    "pd.options.display.max_colwidth = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd6f8c-73ac-4ee2-ab81-cc5b146e49b6",
   "metadata": {},
   "source": [
    "## A sample dataset\n",
    "\n",
    "To demonstrate the pattern, we'll consider an **in-context question answering** use-case where the candidate LLM is presented with both a question and a document/snippet of source text that should include the answer. This is similar to the [Retrieval-Augmented Generation (RAG)](https://aws.amazon.com/what-is/retrieval-augmented-generation/) pattern, but assuming we've already been able to retrieve the relevant source document(s) for our query and are focussing solely on generating a coherent, accurate answer from that context.\n",
    "\n",
    "Specifically, we'll use a small extract of the [Stamford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/) which has already been transformed and pre-processed in [datasets/question-answering/eval-job-input-qa.manifest.jsonl](datasets/question-answering/eval-job-input-qa.manifest.jsonl). See [datasets/Prepare-SQuAD.ipynb](datasets/Prepare-SQuAD.ipynb) for the code that was used to create this extract from the raw public SQuAD source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cec230-1da2-44b7-bb81-2143c47e2632",
   "metadata": {},
   "source": [
    "## The default `QAAccuracy` algorithm\n",
    "\n",
    "To better understand why it might be useful to implement a custom question-answering accuracy evaluator, let's try the fmeval default algorithm first.\n",
    "\n",
    "(See the public fmeval example notebooks e.g. [examples/bedrock-claude-factual-knowledge.ipynb](https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-factual-knowledge.ipynb) for more demos on how to set up model evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14386942-d334-4a74-8881-7ed49133863d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create a BedrockModelRunner with Claude v3's expected API structure:\n",
    "os.environ[\"AWS_REGION\"] = BEDROCK_REGION\n",
    "candidate_model_runner = BedrockModelRunner(\n",
    "    model_id=CANDIDATE_MODEL_ID,\n",
    "    output=\"content[0].text\",\n",
    "    content_template='{\"anthropic_version\": \"bedrock-2023-05-31\", \"max_tokens\": 500, \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": $prompt}]}]}',\n",
    ")\n",
    "\n",
    "# Initialise the built-in QAAccuracy evaluator with default settings:\n",
    "eval_algo = QAAccuracy()\n",
    "\n",
    "# Configure the dataset created in the last notebook:\n",
    "data_config = DataConfig(\n",
    "    dataset_name=\"squad_demo\",\n",
    "    dataset_uri=\"datasets/question-answering/eval-job-input-qa.manifest.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"prompt\",\n",
    "    target_output_location=\"referenceResponse\"\n",
    ")\n",
    "\n",
    "# Run the evaluation and save detailed results to local folder:\n",
    "eval_output = eval_algo.evaluate(\n",
    "    model=candidate_model_runner,\n",
    "    dataset_config=data_config,\n",
    "    prompt_template=\"$model_input\",  # Prompt templating already done in data prep\n",
    "    save=True,\n",
    ")\n",
    "with open(\"/tmp/eval_results/qa_accuracy_squad_demo.jsonl\") as fin:\n",
    "    os.makedirs(\"datasets/eval-local\", exist_ok=True)\n",
    "    with open(\"datasets/eval-local/qa_accuracy_squad_demo.jsonl\", \"w\") as fout:\n",
    "        fout.write(fin.read())\n",
    "eval_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a28444-5043-4728-9e13-10350f755040",
   "metadata": {},
   "source": [
    "Many of the metrics generated by this out-of-the-box evaluator paint a pretty pessimistic view of the model's performance on the task, as shown in the summary below. Only `recall_over_words` gives a high score of ~97%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c2b8df3-2823-4bf4-abc0-23e6f97107f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.17133769728379025\n",
      "exact_match_score: 0.0\n",
      "quasi_exact_match_score: 0.0\n",
      "precision_over_words: 0.10331109925282639\n",
      "recall_over_words: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "for score in eval_output[0].dataset_scores:\n",
    "    print(f\"{score.name}: {score.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f705dc-fc66-4406-bb93-49b9e59567c5",
   "metadata": {},
   "source": [
    "To get some more insight, let's look at a few actual examples from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ecb419-51e9-4187-bf2e-2c23a342706d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 0\n",
      "----------------\n",
      "Target Response: France\n",
      "\n",
      "Model Response:\n",
      "According to the given documentation, Normandy is a region in France. The document states that \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France.\"\n",
      "----------------\n",
      "\n",
      "EXAMPLE 1\n",
      "----------------\n",
      "Target Response: Computational complexity theory\n",
      "\n",
      "Model Response:\n",
      "According to the provided documentation, the branch of theoretical computer science that deals with classifying computational problems by their inherent difficulty and relating those classes to each other is called computational complexity theory.\n",
      "\n",
      "The document states that \"Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other.\"\n",
      "----------------\n",
      "\n",
      "EXAMPLE 2\n",
      "----------------\n",
      "Target Response: SoCal\n",
      "\n",
      "Model Response:\n",
      "According to the documentation, Southern California is often abbreviated as \"SoCal\".\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/question-answering/eval-job-input-qa.manifest.jsonl\") as f:\n",
    "    for ix, line in enumerate(f):\n",
    "        print(f\"EXAMPLE {ix}\\n----------------\")\n",
    "        datum = json.loads(line)\n",
    "        print(f\"Target Response: {datum['referenceResponse']}\\n\")\n",
    "        model_resp = candidate_model_runner.predict(datum[\"prompt\"])\n",
    "        print(f\"Model Response:\\n{model_resp[0]}\\n----------------\\n\")\n",
    "        if ix > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b3acf-0541-4858-ab6b-fd2920938846",
   "metadata": {},
   "source": [
    "Qualitatively, the problem is that while the model is returning *\"correct\"* answers, it's also providing explanations while the reference answers in the dataset are extremely concise. The `recall_over_words` metric is very high because Claude is including the right answer in its response most of the time, but metrics like `f1_score` and `exact_match_score` are very low because Claude is never giving **only** the extracted answer text in the style of the reference answer.\n",
    "\n",
    "For some use-cases, you may **want** this behaviour that penalizes the model for deviations from the expected style of the reference answer(s). In this case, the most likely solution would be to re-engineer the input prompts to explicitly request a more concise response from the model.\n",
    "\n",
    "...But in other cases, you might **allow** these stylistic deviations (and even think the added explanations are useful) - and instead want to evaluate **specific aspects** of the LLM's response like its \"correctness\" or alignment with the reference answer. For this, we can **ask an evaluator LLM** to make a judgement.\n",
    "\n",
    "We can define these custom evaluation algorithms, including ones that might make additional LLM calls to calculate metrics, using the fmeval API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7cd60-e3d1-40d2-a103-04d2448f93c4",
   "metadata": {},
   "source": [
    "## Set up a custom fmeval evaluator\n",
    "\n",
    "In fmeval, custom evaluation algorithms should implement the [EvalAlgorithmInterface](https://github.com/aws/fmeval/blob/5d1af0949f404c522eb37c7567059e37666cfa8b/src/fmeval/eval_algorithms/eval_algorithm.py#L13).\n",
    "\n",
    "The [existing evaluation algorithms](https://github.com/aws/fmeval/tree/main/src/fmeval/eval_algorithms) provide a nice reference to work from when implementing custom algorithms, although many of them (like `QAAccuracy`) are a little complex due to their number of features.\n",
    "\n",
    "The below evaluator is similar to the implementation in [infra/prompt_app/src/datamodel/evaluations/self_critique.py](infra/prompt_app/src/datamodel/evaluations/self_critique.py), used by our prompt engineering app example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aabbb54-26af-4bdc-b235-fbcef270d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Built-Ins:\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from string import Template\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# External Dependencies:\n",
    "from fmeval.constants import DatasetColumns, MEAN\n",
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.data_loaders.util import get_dataset\n",
    "from fmeval.eval_algorithms import EvalAlgorithm, EvalOutput, EvalScore\n",
    "from fmeval.eval_algorithms.eval_algorithm import EvalAlgorithmConfig, EvalAlgorithmInterface\n",
    "from fmeval.eval_algorithms.util import evaluate_dataset, get_dataset_configs, validate_dataset\n",
    "from fmeval.exceptions import EvalAlgorithmClientError\n",
    "from fmeval.model_runners.model_runner import ModelRunner\n",
    "from fmeval.transforms.transform import Transform\n",
    "from fmeval.transforms.transform_pipeline import TransformPipeline\n",
    "from fmeval.transforms.util import validate_call\n",
    "from fmeval.util import get_eval_results_path\n",
    "\n",
    "# Prompt template to ask an LLM to critique correctness of answer vs ground truth:\n",
    "EVAL_TPL = Template(\"\"\"An AI model was asked a question for which the reference correct answer(s) were:\n",
    "\n",
    "<ref-answers>\n",
    "${target}\n",
    "</ref-answers>\n",
    "\n",
    "The model's answer was:\n",
    "\n",
    "<model-answer>\n",
    "${output}\n",
    "</model-answer>\n",
    "\n",
    "Did the model answer correctly in agreement with the provided reference(s)? Answer only Y for yes\n",
    "or N for no, and do not include any other information or reasoning.\n",
    "\"\"\")\n",
    "\n",
    "OUTPUT_KEY = \"llm_judged_accuracy\"\n",
    "\n",
    "class QAAccuracyByLLMScores(Transform):\n",
    "    \"\"\"Scorer inspired by fmeval.eval_algorithms.qa_accuracy.QAAccuracyScores\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_model_runners: List[ModelRunner],\n",
    "        target_output_key: str = DatasetColumns.TARGET_OUTPUT.value.name,\n",
    "        model_output_key: str = DatasetColumns.MODEL_OUTPUT.value.name,\n",
    "        target_output_delimiter: Optional[str] = \"<OR>\",\n",
    "    ):\n",
    "        output_keys = [OUTPUT_KEY]\n",
    "        super().__init__(\n",
    "            eval_model_runners,\n",
    "            target_output_key,\n",
    "            model_output_key,\n",
    "            target_output_delimiter,\n",
    "        )\n",
    "        self.register_input_output_keys(\n",
    "            input_keys=[target_output_key, model_output_key],\n",
    "            output_keys=output_keys,\n",
    "        )\n",
    "        self.target_output_key = target_output_key\n",
    "        self.model_output_key = model_output_key\n",
    "        self.output_keys = output_keys\n",
    "        self.target_output_delimiter = target_output_delimiter\n",
    "\n",
    "        self.eval_model_runners = eval_model_runners\n",
    "        if not (self.eval_model_runners and len(self.eval_model_runners)):\n",
    "            raise EvalAlgorithmClientError(\n",
    "                \"You must provide at least one ModelRunner for LLM-based QA Accuracy evaluation\"\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_score(model_runner: ModelRunner, model_output: str, targets: List[str]) -> float:\n",
    "        prompt = EVAL_TPL.substitute(\n",
    "            target=\"\\n\".join([f\"<ref-answer>{t}</ref-answer>\" for t in targets]),\n",
    "            output=model_output,\n",
    "        )\n",
    "        eval_resp, logprobs = model_runner.predict(prompt)\n",
    "        eval_resp = eval_resp.strip().upper()\n",
    "        if not len(eval_resp):\n",
    "            return 0.5  # Swallow unexpected evaluation result & return 'not sure'\n",
    "        elif eval_resp[0] == \"Y\":\n",
    "            return 1.0\n",
    "        elif eval_resp[0] == \"N\":\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 0.5  # Swallow unexpected evaluation result & return 'not sure'\n",
    "\n",
    "    @validate_call\n",
    "    def __call__(self, record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        model_output = record[self.model_output_key]\n",
    "        target_outputs = record[self.target_output_key].split(self.target_output_delimiter)\n",
    "\n",
    "        # Return average score across all evaluator models\n",
    "        record[OUTPUT_KEY] = sum(\n",
    "            self._get_score(model_runner=runner, model_output=model_output, targets=target_outputs)\n",
    "            for runner in self.eval_model_runners\n",
    "        ) / len(self.eval_model_runners)\n",
    "        # sum(score_fn_override(runner, model_output, possible_targets) for runner in model_runners) / len(model_runners)\n",
    "        return record\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class QAAccuracyByLLMConfig(EvalAlgorithmConfig):\n",
    "    \"\"\"Configuration for the QA Accuracy Evaluation\n",
    "\n",
    "    :param eval_model_runners: The QAAccuracyByLLM evaluator uses one or more LLMs to judge whether the answer\n",
    "        answer generated by the model under test is in agreement with the reference answers. Therefore you need to\n",
    "        provide one or more ModelRunner instances to use for the evaluation.\n",
    "    :param target_output_delimiter: Target Output can have multiple answers. We expect customer to combine all the\n",
    "        possible answers into a single string and use the delimiter to separate them. For instance,\n",
    "        if the answers are [\"UK\", \"England\"] and the delimiter=\"<OR>\", then the target_output should be \"UK<OR>England\".\n",
    "    \"\"\"\n",
    "\n",
    "    eval_model_runners: List[ModelRunner]\n",
    "    target_output_delimiter: Optional[str] = \"<OR>\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if not len(self.eval_model_runners):\n",
    "            raise EvalAlgorithmClientError(\n",
    "                \"You must provide at least one ModelRunner for LLM-based QA Accuracy evaluation\"\n",
    "            )\n",
    "        if self.target_output_delimiter == \"\":\n",
    "            raise EvalAlgorithmClientError(\n",
    "                \"Empty target_output_delimiter is provided. Please either provide a non-empty string, or set it to None.\"\n",
    "            )\n",
    "\n",
    "\n",
    "class QAAccuracyByLLM(EvalAlgorithmInterface):\n",
    "    \"\"\"This evaluation measures question answering (QA) performance via critique from LLM(s)\n",
    "\n",
    "    The code is closely aligned to fmeval's vanilla `QAAccuracy` eval algorithm, since the actual\n",
    "    logic is implemented in the `QAAccuracyByLLMScores` transformer. We had to re-implement (rather\n",
    "    than re-using the QAAccuracy) because of the way constants like the list of score names are\n",
    "    referenced in the upstream.\n",
    "\n",
    "    This evaluator outputs one metric only: The mean of the judged 0-0.5-1 response quality judged\n",
    "    by the panel of (potentially multiple) evaluator model runners.\n",
    "    \"\"\"\n",
    "\n",
    "    eval_name = \"qa_accuracy_by_llm\"\n",
    "\n",
    "    def __init__(self, eval_algorithm_config: QAAccuracyByLLMConfig):\n",
    "        super().__init__(eval_algorithm_config)\n",
    "        self._eval_algorithm_config = eval_algorithm_config\n",
    "        self.transform = QAAccuracyByLLMScores(\n",
    "            eval_model_runners=eval_algorithm_config.eval_model_runners,\n",
    "            target_output_delimiter=eval_algorithm_config.target_output_delimiter,\n",
    "        )\n",
    "\n",
    "    def evaluate_sample(self, target_output: str, model_output: str) -> List[EvalScore]:\n",
    "        \"\"\"Compute QA accuracy metrics for a single sample.\n",
    "\n",
    "        :param target_output: The expected/desired model output.\n",
    "        :param model_output: The actual model output.\n",
    "        :returns: A list of EvalScore objects, one for each of the QA accuracy metrics.\n",
    "        \"\"\"\n",
    "        target_output_key = self.transform.target_output_key\n",
    "        model_output_key = self.transform.model_output_key\n",
    "        sample = {target_output_key: target_output, model_output_key: model_output}\n",
    "        pipeline = TransformPipeline([self.transform])\n",
    "        result = pipeline.execute_record(sample)\n",
    "        return [EvalScore(name=score_name, value=result[score_name]) for score_name in self.transform.output_keys]\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        model: Optional[ModelRunner] = None,\n",
    "        dataset_config: Optional[DataConfig] = None,\n",
    "        prompt_template: Optional[str] = None,\n",
    "        num_records: int = 100,\n",
    "        save: bool = False,\n",
    "    ) -> List[EvalOutput]:\n",
    "        dataset_configs = get_dataset_configs(dataset_config, self.eval_name)\n",
    "        eval_outputs = []\n",
    "        for dataset_config in dataset_configs:\n",
    "            dataset = get_dataset(dataset_config, num_records)\n",
    "            validate_dataset(dataset, [DatasetColumns.TARGET_OUTPUT.value.name])\n",
    "            eval_output = evaluate_dataset(\n",
    "                dataset=dataset,\n",
    "                pipeline=TransformPipeline([self.transform]),\n",
    "                dataset_name=dataset_config.dataset_name,\n",
    "                eval_name=self.eval_name,\n",
    "                metric_names=self.transform.output_keys,\n",
    "                eval_results_path=get_eval_results_path(),\n",
    "                model=model,\n",
    "                prompt_template=prompt_template,\n",
    "                agg_method=MEAN,\n",
    "                save=save,\n",
    "            )\n",
    "            eval_outputs.append(eval_output)\n",
    "        return eval_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78321d1a-4bdd-482e-968f-7f543804f12d",
   "metadata": {},
   "source": [
    "## Running the custom evaluation\n",
    "\n",
    "With the custom evaluation algorithm defined, we can run a job similarly to before but substitute our own algorithm instead.\n",
    "\n",
    "Note that since this algorithm uses a (panel of) LLM(s) to evaluate the response from the model under test, it requires additional `BedrockModelRunner`(s) in the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e060a79-eed4-4318-b16b-e04e800855d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create a BedrockModelRunner for the evaluator model:\n",
    "eval_model_runner = BedrockModelRunner(\n",
    "    model_id=EVAL_MODEL_ID,\n",
    "    output=\"content[0].text\",\n",
    "    content_template='{\"anthropic_version\": \"bedrock-2023-05-31\", \"max_tokens\": 500, \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": $prompt}]}]}',\n",
    ")\n",
    "\n",
    "# Initialise the custom evaluation algorithm with the relevant config:\n",
    "eval_by_llm_algo = QAAccuracyByLLM(QAAccuracyByLLMConfig(eval_model_runners=[eval_model_runner]))\n",
    "\n",
    "# Run the evaluation and save detailed results to local folder:\n",
    "eval_by_llm_output = eval_by_llm_algo.evaluate(\n",
    "    model=candidate_model_runner,\n",
    "    dataset_config=data_config,\n",
    "    prompt_template=\"$model_input\",  # Prompt templating already done in data prep\n",
    "    save=True,\n",
    ")\n",
    "with open(\"/tmp/eval_results/qa_accuracy_by_llm_squad_demo.jsonl\") as fin:\n",
    "    os.makedirs(\"datasets/eval-local\", exist_ok=True)\n",
    "    with open(\"datasets/eval-local/qa_accuracy_by_llm_squad_demo.jsonl\", \"w\") as fout:\n",
    "        fout.write(fin.read())\n",
    "eval_by_llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcc724-fd55-45f0-b072-5e4d19e41d35",
   "metadata": {},
   "source": [
    "The LLM-based evaluation, since it's only checking specifically for correctness of the generated answer in line with the reference answer, is much more generous and in line with our human perception of the model's performance for the task.\n",
    "\n",
    "In this particular case, where we'd like to evaluate the correctness of models' answers against a ground truth independently of the \"style\" of the answer, this provides a more useful view of performance than default token-based metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb437de-9948-408f-8333-79ac495ad8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_judged_accuracy: 0.9142857142857143\n"
     ]
    }
   ],
   "source": [
    "for score in eval_by_llm_output[0].dataset_scores:\n",
    "    print(f\"{score.name}: {score.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67b760-df3d-4e53-b42b-f4a4765a42aa",
   "metadata": {},
   "source": [
    "## Exploring results in detail\n",
    "\n",
    "The detailed JSON-Lines result files also allow exploring results at the record level if required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faeecac5-d7c1-4a30-bf95-363c246b57bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_output</th>\n",
       "      <th>model_output</th>\n",
       "      <th>llm_judged_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>October&lt;OR&gt;October 1973&lt;OR&gt;1973</td>\n",
       "      <td>According to the documentation, the 1973 oil crisis began in Octob...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Construction</td>\n",
       "      <td>Based on the documentation provided, the process of constructing a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paul Baran developed the concept Distributed Adaptive Message Bloc...</td>\n",
       "      <td>According to the documentation, Paul Baran developed the concept o...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Computational complexity theory</td>\n",
       "      <td>According to the provided documentation, the branch of theoretical...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>diversified&lt;OR&gt;highly diversified</td>\n",
       "      <td>According to the documentation provided, the economy of Victoria i...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1998&lt;OR&gt;Following a referendum in 1997</td>\n",
       "      <td>According to the documentation, the current Parliament of Scotland...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Central Asia&lt;OR&gt;the arid plains of Central Asia</td>\n",
       "      <td>According to the AnyCompany documentation, the Black Death is thou...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Latin</td>\n",
       "      <td>According to the documentation provided, the word \"imperialism\" or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the concept of force&lt;OR&gt;force</td>\n",
       "      <td>According to the documentation, philosophers in antiquity used the...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>According to the documentation provided, Normandy is located in Fr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>According to the documentation provided, the atomic number of oxyg...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BSkyB</td>\n",
       "      <td>According to the documentation provided, the company formed by the...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Amazonia&lt;OR&gt;Amazonia or the Amazon Jungle&lt;OR&gt;also known in English...</td>\n",
       "      <td>According to the documentation, the Amazon rainforest is also know...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>solar&lt;OR&gt;solar power, nuclear power or geothermal energy&lt;OR&gt;solar ...</td>\n",
       "      <td>According to the documentation, a notable non-combustion heat sour...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SoCal</td>\n",
       "      <td>According to the documentation, Southern California is often abbre...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>According to the information provided in the documentation, Jackso...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Warsaw</td>\n",
       "      <td>Based on the information provided in the document, the largest cit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>John Harvard</td>\n",
       "      <td>According to the documentation, Harvard is named after John Harvar...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Islamism</td>\n",
       "      <td>Based on the information provided in the document, an Islamic revi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>itself</td>\n",
       "      <td>According to the AnyCompany documentation, the only divisors of a ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the North Sea in the Netherlands&lt;OR&gt;North Sea</td>\n",
       "      <td>According to the documentation, the Rhine empties into the North S...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>about one-eighth&lt;OR&gt;southern and central parts of France,&lt;OR&gt;the s...</td>\n",
       "      <td>According to the documentation, the Huguenot population in France ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>its root word pharma&lt;OR&gt;pharma</td>\n",
       "      <td>According to the documentation provided, the word \"pharmacy\" is de...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40%&lt;OR&gt;40</td>\n",
       "      <td>According to the information provided in the document, the richest...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1754–1763</td>\n",
       "      <td>According to the documentation provided, the French and Indian War...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>comb jellies&lt;OR&gt;phylum of animals that live in marine waters&lt;OR&gt;a ...</td>\n",
       "      <td>According to the provided documentation, a ctenophore (plural: cte...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the United Nations</td>\n",
       "      <td>According to the documentation provided, the Intergovernmental Pan...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>a body of treaties and legislation, such as Regulations and Direct...</td>\n",
       "      <td>Based on the provided documentation, European Union law can be sum...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>private research&lt;OR&gt;private research university&lt;OR&gt;a private resea...</td>\n",
       "      <td>According to the documentation, the University of Chicago is a pri...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Civil disobedience</td>\n",
       "      <td>According to the documentation provided, the term used to describe...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>melt (magma and/or lava)&lt;OR&gt;melt&lt;OR&gt;rock crystallizes from melt (m...</td>\n",
       "      <td>According to the AnyCompany documentation provided, an igneous roc...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>disease</td>\n",
       "      <td>According to the documentation provided, the immune system protect...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Yuán Cháo&lt;OR&gt;元朝</td>\n",
       "      <td>According to the documentation provided, the Chinese name for the ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Fresno</td>\n",
       "      <td>According to the information provided in the document, Fresno is t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>independent schools&lt;OR&gt;independent</td>\n",
       "      <td>According to the documentation provided, another name for private ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            target_output  \\\n",
       "0                                         October<OR>October 1973<OR>1973   \n",
       "1                                                            Construction   \n",
       "2   Paul Baran developed the concept Distributed Adaptive Message Bloc...   \n",
       "3                                         Computational complexity theory   \n",
       "4                                       diversified<OR>highly diversified   \n",
       "5                                  1998<OR>Following a referendum in 1997   \n",
       "6                         Central Asia<OR>the arid plains of Central Asia   \n",
       "7                                                                   Latin   \n",
       "8                                           the concept of force<OR>force   \n",
       "9                                                                  France   \n",
       "10                                                                      8   \n",
       "11                                                                  BSkyB   \n",
       "12  Amazonia<OR>Amazonia or the Amazon Jungle<OR>also known in English...   \n",
       "13  solar<OR>solar power, nuclear power or geothermal energy<OR>solar ...   \n",
       "14                                                                  SoCal   \n",
       "15                                                           Jacksonville   \n",
       "16                                                                 Warsaw   \n",
       "17                                                           John Harvard   \n",
       "18                                                               Islamism   \n",
       "19                                                                 itself   \n",
       "20                          the North Sea in the Netherlands<OR>North Sea   \n",
       "21  about one-eighth<OR>southern and central parts of France,<OR>the s...   \n",
       "22                                         its root word pharma<OR>pharma   \n",
       "23                                                              40%<OR>40   \n",
       "24                                                              1754–1763   \n",
       "25  comb jellies<OR>phylum of animals that live in marine waters<OR>a ...   \n",
       "26                                                     the United Nations   \n",
       "27  a body of treaties and legislation, such as Regulations and Direct...   \n",
       "28  private research<OR>private research university<OR>a private resea...   \n",
       "29                                                     Civil disobedience   \n",
       "30  melt (magma and/or lava)<OR>melt<OR>rock crystallizes from melt (m...   \n",
       "31                                                                disease   \n",
       "32                                                        Yuán Cháo<OR>元朝   \n",
       "33                                                                 Fresno   \n",
       "34                                     independent schools<OR>independent   \n",
       "\n",
       "                                                             model_output  \\\n",
       "0   According to the documentation, the 1973 oil crisis began in Octob...   \n",
       "1   Based on the documentation provided, the process of constructing a...   \n",
       "2   According to the documentation, Paul Baran developed the concept o...   \n",
       "3   According to the provided documentation, the branch of theoretical...   \n",
       "4   According to the documentation provided, the economy of Victoria i...   \n",
       "5   According to the documentation, the current Parliament of Scotland...   \n",
       "6   According to the AnyCompany documentation, the Black Death is thou...   \n",
       "7   According to the documentation provided, the word \"imperialism\" or...   \n",
       "8   According to the documentation, philosophers in antiquity used the...   \n",
       "9   According to the documentation provided, Normandy is located in Fr...   \n",
       "10  According to the documentation provided, the atomic number of oxyg...   \n",
       "11  According to the documentation provided, the company formed by the...   \n",
       "12  According to the documentation, the Amazon rainforest is also know...   \n",
       "13  According to the documentation, a notable non-combustion heat sour...   \n",
       "14  According to the documentation, Southern California is often abbre...   \n",
       "15  According to the information provided in the documentation, Jackso...   \n",
       "16  Based on the information provided in the document, the largest cit...   \n",
       "17  According to the documentation, Harvard is named after John Harvar...   \n",
       "18  Based on the information provided in the document, an Islamic revi...   \n",
       "19  According to the AnyCompany documentation, the only divisors of a ...   \n",
       "20  According to the documentation, the Rhine empties into the North S...   \n",
       "21  According to the documentation, the Huguenot population in France ...   \n",
       "22  According to the documentation provided, the word \"pharmacy\" is de...   \n",
       "23  According to the information provided in the document, the richest...   \n",
       "24  According to the documentation provided, the French and Indian War...   \n",
       "25  According to the provided documentation, a ctenophore (plural: cte...   \n",
       "26  According to the documentation provided, the Intergovernmental Pan...   \n",
       "27  Based on the provided documentation, European Union law can be sum...   \n",
       "28  According to the documentation, the University of Chicago is a pri...   \n",
       "29  According to the documentation provided, the term used to describe...   \n",
       "30  According to the AnyCompany documentation provided, an igneous roc...   \n",
       "31  According to the documentation provided, the immune system protect...   \n",
       "32  According to the documentation provided, the Chinese name for the ...   \n",
       "33  According to the information provided in the document, Fresno is t...   \n",
       "34  According to the documentation provided, another name for private ...   \n",
       "\n",
       "    llm_judged_accuracy  \n",
       "0                   1.0  \n",
       "1                   1.0  \n",
       "2                   1.0  \n",
       "3                   1.0  \n",
       "4                   1.0  \n",
       "5                   0.0  \n",
       "6                   1.0  \n",
       "7                   1.0  \n",
       "8                   1.0  \n",
       "9                   1.0  \n",
       "10                  1.0  \n",
       "11                  1.0  \n",
       "12                  1.0  \n",
       "13                  1.0  \n",
       "14                  1.0  \n",
       "15                  1.0  \n",
       "16                  1.0  \n",
       "17                  1.0  \n",
       "18                  1.0  \n",
       "19                  1.0  \n",
       "20                  1.0  \n",
       "21                  1.0  \n",
       "22                  1.0  \n",
       "23                  1.0  \n",
       "24                  1.0  \n",
       "25                  1.0  \n",
       "26                  1.0  \n",
       "27                  1.0  \n",
       "28                  1.0  \n",
       "29                  1.0  \n",
       "30                  1.0  \n",
       "31                  0.0  \n",
       "32                  0.0  \n",
       "33                  1.0  \n",
       "34                  1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_records = []\n",
    "with open(\"datasets/eval-local/qa_accuracy_by_llm_squad_demo.jsonl\") as f:\n",
    "    for line in f:\n",
    "        datum = json.loads(line)\n",
    "        out_records.append({\n",
    "            \"target_output\": datum[\"target_output\"],\n",
    "            \"model_output\": datum[\"model_output\"],\n",
    "            \"llm_judged_accuracy\": next(s for s in datum[\"scores\"] if s[\"name\"] == \"llm_judged_accuracy\")[\"value\"],\n",
    "        })\n",
    "out_df = pd.DataFrame(out_records)\n",
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bddbf8df-90f4-4a9f-85d7-64a4a0259383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "------\n",
      "1998<OR>Following a referendum in 1997\n",
      "\n",
      "Actual\n",
      "------\n",
      "According to the documentation, the current Parliament of Scotland was convened by the Scotland Act 1998, which sets out its powers as a devolved legislature. The first meeting of the new Parliament took place on 12 May 1999.\n"
     ]
    }
   ],
   "source": [
    "item = out_df[out_df[\"llm_judged_accuracy\"] == 0].iloc[0]\n",
    "\n",
    "print(f\"Target\\n------\\n{item.target_output}\")\n",
    "print(f\"\\nActual\\n------\\n{item.model_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f6840-bd38-424e-9402-2cfa91efc61b",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "While the `fmeval` library provides a range of pre-built evaluation metrics for foundation model evaluation, the open design also supports bringing custom algorithms. This can be extended to include LLM-critique-based evaluations, which can be especially useful in cases where *particular aspects* of the response need to be evaluated - but are hard to describe using traditional pattern matching or text processing methods alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

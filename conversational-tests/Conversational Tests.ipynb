{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740b5c9f-0a9b-41d6-8cde-fb314e0e6702",
   "metadata": {},
   "source": [
    "# Conversational tests with `agent-evaluation`\n",
    "\n",
    "> *This notebook has been tested in the Python 3 kernel of SageMaker Studio JupyterLab (Distribution v1.9)*\n",
    "\n",
    "In this notebook, we'll show how you can use AWS Labs' open-source [`agent-evaluation` framework](https://awslabs.github.io/agent-evaluation/) to validate integrated agent systems perform as expected over multi-turn conversations - with expectations defined in natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b378fc3-66aa-4e72-a181-2f5e539fdcfb",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To get started, we'll first need to install the framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde6551-3339-42fa-b7b5-e1adebe9b9b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Force Pydantic for https://github.com/aws/sagemaker-distribution/issues/436\n",
    "%pip install agent-evaluation \"pydantic>=2.8,<2.9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275d3e9-6946-4b42-b0f3-064a2462eb6d",
   "metadata": {},
   "source": [
    "`agent-evaluation` supports evaluating a range of [\"target\"](https://awslabs.github.io/agent-evaluation/targets/) types, including:\n",
    "\n",
    "- Amazon Bedrock Agents and Knowledge Bases\n",
    "- Amazon Q for Business\n",
    "- Amazon SageMaker Endpoints\n",
    "- Custom targets\n",
    "\n",
    "In this example, we'll use the **Knowledge Base for Amazon Bedrock** created in the [../RAG (Bedrock and Ragas).ipynb notebook](../RAG%20(Bedrock%20and%20Ragas).ipynb).\n",
    "\n",
    "▶️ **Follow** the instructions in the other notebook to create the sample Bedrock KB, if you haven't already\n",
    "\n",
    "▶️ **Replace** the 'TODO' in the code cell below with the *auto-generated ID* of your Bedrock KB, which should be a short alphanumeric string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a4bf5-8392-450e-af6a-f0822538f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_id = \"TODO\"  # For example \"G6GPI4YRUW\"\n",
    "generate_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf7dac-bb23-486d-adbf-15cd70cc630c",
   "metadata": {},
   "source": [
    "## Define the test scripts\n",
    "\n",
    "While data scientists may be familiar with using validation datasets to calculate **representative metrics** describing aggregate model quality across many samples, software engineers might be more used to defining **test-cases** to confirm a module or system performs as expected across a range of scenarios.\n",
    "\n",
    "In LLM-based application engineering, both of these perspectives are valuable:\n",
    "\n",
    "- When we have a sufficiently large and **real-world-representative** dataset available, a metrics-based approach can quantify average system performance across common use-cases: Giving us a useful target to optimize towards.\n",
    "- ...But in the **early stages** of a project, where we might be much less confident what queries are common or rare, it might be more natural to take a *test-case-based approach* and expect our system to pass 100% of defined example journeys.\n",
    "\n",
    "With the `agent-evaluation` framework:\n",
    "\n",
    "- Builders [write test cases](https://awslabs.github.io/agent-evaluation/user_guide/#writing-test-cases) in a [YAML](https://en.wikipedia.org/wiki/YAML)-based format, and use the framework to run the test cases and report on successes & failures.\n",
    "- Test-cases can be **multi-turn conversations**, supporting end-to-end testing of more complex user journeys\n",
    "- Actual system inputs (user messages) are **generated** based on your specifications by an LLM, not taken verbatim from your test plan... So if the exact wording of your question is important, be specific!\n",
    "- System outputs (bot responses) are also **judged** against your provided expectations by an LLM, so you can be flexible (but should be specific) when describing what behaviour you want to see.\n",
    "\n",
    "In this example, we've already [initialized a template test plan](https://awslabs.github.io/agent-evaluation/user_guide) for you in [agenteval.tpl.yml](agenteval.tpl.yml).\n",
    "\n",
    "Run the cell below to generate the final `agenteval.yml` with your Bedrock KB and generator model ID populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca43805-140e-47ab-820d-4bd25b54bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"agenteval.tpl.yml\") as ftpl:\n",
    "    test_spec_str = ftpl.read()\n",
    "\n",
    "test_spec_str = test_spec_str.replace(\"${kb_model_id}\", generate_model_id)\n",
    "test_spec_str = test_spec_str.replace(\"${kb_id}\", knowledge_base_id)\n",
    "\n",
    "with open(\"agenteval.yml\", \"w\") as fspec:\n",
    "    fspec.write(\"#### AUTO-GENERATED FILE - Edit agenteval.tpl.yaml instead! ####\\n\")\n",
    "    fspec.write(test_spec_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0ebb1-0eb4-4e40-98ee-52e367e566b3",
   "metadata": {},
   "source": [
    "## Run the tests\n",
    "\n",
    "The [agenteval CLI](https://awslabs.github.io/agent-evaluation/cli/) can run your tests with multi-threading, report generation, and conditional return codes: Great for integrating to CI/CD workflows, and similar to conventional test automation tools like [pytest](https://docs.pytest.org/):\n",
    "\n",
    "> ℹ️ **Remember**: If you edited `agenteval.tpl.yml`, you'll need to re-run the cell above to refresh your `agenteval.yml`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1a0c6-56ac-4b9b-ae4c-85e8b19ea41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!agenteval run --plan-dir . --num-threads 8 --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f357df8-538c-40db-8223-794d0b090725",
   "metadata": {},
   "source": [
    "As well as the test result logs, you should see the framework created:\n",
    "\n",
    "- [agenteval_summary.md](agenteval_summary.md) - a **human-readable report** in [Markdown](https://en.wikipedia.org/wiki/Markdown) format\n",
    "- [agnteval_traces](agenteval_traces) folder - including **structured JSON files** per test-case, with full details of the reasoning chain and agent/evaluator invocations taken\n",
    "\n",
    "▶️ Did the tests all pass as expected? **Check** the summary and trace JSONs: What were the *actual input messages* sent to the Bedrock KB for each test case?\n",
    "\n",
    "▶️ **Why** is the second step of the `amazon_followup` test case structured the way it is? What happens if you replace it with simply `Ask the agent how many trees are in it`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da43bf-cfac-41d2-9c9a-6cd45a7a9820",
   "metadata": {},
   "source": [
    "## Clean-Up\n",
    "\n",
    "Once you're done experimenting, refer to the *Clean-Up section* of the [../RAG (Bedrock and Ragas).ipynb notebook](../RAG%20(Bedrock%20and%20Ragas).ipynb) for steps to delete your Bedrock Knowledge Base, to avoid ongoing charges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14609b05-ad77-4129-bc7e-0ad64b252f8f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "[AWS Labs' agent-evaluation](https://awslabs.github.io/agent-evaluation) provides an open-source, conversational test-case-based framework for validating that LLM-based systems complete example user journeys as expected - similar to traditional integration test frameworks for software engineering.\n",
    "\n",
    "It can integrate with a range of orchestration tools like Amazon Bedrock Agents and Amazon Q for Business, but also supports [custom targets](https://awslabs.github.io/agent-evaluation/targets/custom_targets/) for you to connect to other integrated systems as needed.\n",
    "\n",
    "Interestingly, both the generation of input messages and the evaluation of outputs against your listed criteria are **LLM-powered**: So you should be careful when writing your test plan to avoid ambiguity in what the input messages should be or what is and isn't acceptable for a response.\n",
    "\n",
    "These kinds of tests can be especially useful **early on** in your application building journey, when you might not have a clear idea of which journeys will be most common or a large dataset of example messages to draw on. As you build up a more mature understanding of this distribution and a bigger dataset of examples, it may be useful to transition to more **metrics-based** evaluation: where some test failures are expected and builders work to *increase overall pass rate* rather than to *retain 100% success*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
